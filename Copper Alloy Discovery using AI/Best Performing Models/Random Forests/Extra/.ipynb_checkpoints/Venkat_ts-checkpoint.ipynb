{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120.   120.  ]\n",
      " [ 33.33  32.  ]\n",
      " [138.07 152.  ]\n",
      " [ 69.57  72.  ]\n",
      " [ 74.23  71.  ]\n",
      " [ 48.75  55.  ]\n",
      " [117.9   89.  ]\n",
      " [130.   105.  ]\n",
      " [ 49.35  45.  ]\n",
      " [ 46.96  55.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 47.83  45.  ]\n",
      " [ 58.24  58.  ]\n",
      " [ 50.93  51.  ]\n",
      " [ 32.95  32.  ]\n",
      " [ 54.48  70.  ]\n",
      " [ 39.99  40.  ]\n",
      " [ 81.13  85.  ]\n",
      " [ 55.    55.  ]\n",
      " [ 81.84  80.  ]\n",
      " [ 77.12  95.  ]\n",
      " [ 36.03  36.  ]\n",
      " [ 66.19  80.  ]\n",
      " [ 57.83  59.  ]\n",
      " [ 57.12  57.  ]\n",
      " [ 50.99  45.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 32.01  32.  ]\n",
      " [ 44.53  44.  ]\n",
      " [ 49.59  45.  ]\n",
      " [ 57.12  57.  ]\n",
      " [127.84 130.  ]\n",
      " [ 51.06  51.  ]\n",
      " [ 38.66  38.  ]\n",
      " [101.18 110.  ]\n",
      " [ 32.04  32.  ]\n",
      " [105.7  115.  ]\n",
      " [ 48.79  53.  ]\n",
      " [119.04 124.  ]\n",
      " [ 53.75  50.  ]\n",
      " [ 43.98  30.  ]\n",
      " [141.63 142.  ]\n",
      " [ 66.01  66.  ]\n",
      " [ 55.51  54.  ]\n",
      " [ 42.68  44.  ]\n",
      " [ 85.17  85.  ]\n",
      " [ 56.31  52.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 55.44  59.  ]\n",
      " [ 49.01  49.  ]\n",
      " [ 53.2   51.  ]\n",
      " [ 58.06  68.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 32.24  36.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 49.21  49.  ]\n",
      " [ 88.63  98.  ]\n",
      " [ 42.43  30.  ]\n",
      " [ 77.99  57.  ]\n",
      " [ 32.21  34.  ]\n",
      " [ 70.38  62.  ]\n",
      " [111.33  80.  ]\n",
      " [ 38.09  38.  ]\n",
      " [ 59.33  78.  ]\n",
      " [ 44.52  35.  ]\n",
      " [ 60.65  69.  ]\n",
      " [ 86.51  79.  ]\n",
      " [107.06 108.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 42.32  45.  ]\n",
      " [ 56.56  57.  ]\n",
      " [ 62.18  61.  ]\n",
      " [ 69.88  70.  ]\n",
      " [ 56.43  59.  ]\n",
      " [ 59.85  40.  ]\n",
      " [ 77.67  77.  ]\n",
      " [ 32.74  32.  ]\n",
      " [ 49.43  51.  ]\n",
      " [ 46.91  50.  ]\n",
      " [ 38.01  38.  ]\n",
      " [ 60.96  58.5 ]\n",
      " [ 32.    34.  ]\n",
      " [ 38.09  38.  ]\n",
      " [ 89.66 100.  ]\n",
      " [ 32.07  32.  ]\n",
      " [ 42.01  45.  ]\n",
      " [ 40.85  45.  ]\n",
      " [196.82 195.  ]\n",
      " [ 49.04  45.  ]\n",
      " [ 50.01  45.  ]\n",
      " [ 56.71  52.  ]\n",
      " [ 72.05  61.  ]\n",
      " [ 79.46  78.  ]\n",
      " [ 51.31  45.  ]\n",
      " [ 45.41  44.  ]\n",
      " [ 53.36  84.  ]\n",
      " [ 38.05  38.  ]\n",
      " [ 65.07  72.  ]\n",
      " [204.65 212.  ]\n",
      " [ 50.78  55.  ]\n",
      " [ 86.3   72.  ]\n",
      " [ 46.75  50.  ]\n",
      " [ 53.68  56.  ]\n",
      " [ 53.19  55.  ]\n",
      " [ 85.7   82.  ]\n",
      " [ 81.1   85.  ]\n",
      " [ 63.88  60.  ]\n",
      " [ 50.01  48.  ]\n",
      " [ 47.3   47.  ]\n",
      " [109.66 100.  ]\n",
      " [ 50.43  50.  ]\n",
      " [ 86.13  87.  ]\n",
      " [ 87.69  88.  ]\n",
      " [ 75.04  89.  ]\n",
      " [ 69.9   80.  ]\n",
      " [ 55.48  56.  ]\n",
      " [ 53.25  50.  ]\n",
      " [ 49.09  49.  ]\n",
      " [ 74.15  83.  ]\n",
      " [ 66.61  65.  ]\n",
      " [ 58.06  54.  ]\n",
      " [ 69.6   78.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 69.25  79.  ]\n",
      " [ 55.14  55.  ]\n",
      " [ 63.74  67.  ]\n",
      " [ 78.68  90.  ]\n",
      " [ 76.49  67.  ]\n",
      " [ 40.94  41.  ]\n",
      " [ 53.36  55.  ]\n",
      " [ 35.1   32.  ]\n",
      " [ 51.74  52.  ]\n",
      " [ 36.    36.  ]\n",
      " [ 68.9   70.  ]\n",
      " [107.01 105.  ]\n",
      " [ 86.3   76.  ]\n",
      " [ 49.06  42.  ]\n",
      " [180.71 178.  ]\n",
      " [ 78.71  77.  ]\n",
      " [ 54.37  58.5 ]\n",
      " [ 50.71  66.  ]\n",
      " [ 47.49  45.  ]\n",
      " [ 50.16  50.  ]\n",
      " [ 42.    42.  ]\n",
      " [ 35.    35.  ]\n",
      " [ 67.76  66.  ]\n",
      " [ 51.45  52.  ]\n",
      " [ 48.69  50.  ]\n",
      " [ 39.99  40.  ]\n",
      " [101.41 115.  ]\n",
      " [ 72.37  72.  ]\n",
      " [ 37.83  38.  ]\n",
      " [ 33.55  32.  ]\n",
      " [ 49.88  48.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 76.82  74.  ]\n",
      " [ 56.84  48.  ]\n",
      " [ 49.64  50.  ]\n",
      " [ 51.78  55.  ]\n",
      " [ 74.17  80.  ]\n",
      " [ 52.26  52.  ]\n",
      " [ 55.11  55.  ]\n",
      " [ 38.09  37.  ]\n",
      " [ 39.92  40.  ]\n",
      " [ 32.06  32.  ]\n",
      " [ 61.4   61.  ]\n",
      " [ 69.98  63.  ]\n",
      " [ 52.27  38.  ]\n",
      " [ 40.88  34.  ]\n",
      " [ 38.04  38.  ]\n",
      " [ 34.    34.  ]\n",
      " [ 74.15  72.  ]\n",
      " [ 85.39  85.  ]\n",
      " [ 50.93  45.  ]\n",
      " [ 32.03  32.  ]\n",
      " [ 69.89  71.  ]\n",
      " [ 73.63 107.  ]\n",
      " [ 58.34  57.  ]\n",
      " [ 59.43  45.  ]\n",
      " [124.39 102.  ]\n",
      " [ 50.38  48.  ]\n",
      " [ 54.95  55.  ]\n",
      " [ 34.    34.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 50.16  45.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 53.36  61.  ]\n",
      " [ 81.51  83.  ]\n",
      " [ 62.55  62.5 ]\n",
      " [ 32.    32.  ]\n",
      " [ 64.45  70.  ]\n",
      " [ 54.25  54.  ]\n",
      " [ 32.02  32.  ]\n",
      " [ 72.81  63.  ]\n",
      " [ 70.06  67.  ]\n",
      " [105.64 115.  ]\n",
      " [ 47.8   48.  ]\n",
      " [ 72.32  72.  ]\n",
      " [ 93.99  94.  ]\n",
      " [ 47.83  50.  ]\n",
      " [ 72.07  83.  ]\n",
      " [ 34.45 116.  ]\n",
      " [ 60.65  52.  ]\n",
      " [ 77.06  70.  ]\n",
      " [ 49.06  40.  ]\n",
      " [ 76.    72.  ]\n",
      " [ 54.09  54.  ]\n",
      " [ 62.23  67.  ]\n",
      " [ 71.87  70.  ]\n",
      " [ 68.35  60.  ]\n",
      " [ 62.55  58.  ]\n",
      " [193.38 200.  ]\n",
      " [ 58.1   58.  ]\n",
      " [ 32.02  32.  ]\n",
      " [ 69.6   73.  ]\n",
      " [ 63.93  50.  ]\n",
      " [118.06 116.  ]\n",
      " [ 34.    34.  ]\n",
      " [ 34.04  34.  ]\n",
      " [ 92.66 105.  ]\n",
      " [ 45.48  47.  ]\n",
      " [ 85.43  86.  ]\n",
      " [ 43.84  34.  ]\n",
      " [ 50.42  51.  ]\n",
      " [ 47.33  47.  ]\n",
      " [ 42.41  37.  ]\n",
      " [ 32.02  32.  ]\n",
      " [109.97 100.  ]\n",
      " [ 83.73 107.  ]\n",
      " [180.71 183.  ]\n",
      " [ 69.49  60.  ]\n",
      " [ 49.8   48.  ]\n",
      " [ 66.    66.  ]\n",
      " [ 53.2   55.  ]\n",
      " [ 97.86 110.  ]\n",
      " [ 75.04  84.  ]\n",
      " [ 49.52  47.  ]\n",
      " [ 42.57  43.  ]\n",
      " [ 60.96  52.  ]\n",
      " [ 32.    32.  ]\n",
      " [ 60.87  58.  ]\n",
      " [105.18 100.  ]\n",
      " [ 33.83  33.  ]\n",
      " [ 78.54  84.  ]\n",
      " [ 70.8   38.  ]\n",
      " [ 74.38  90.  ]\n",
      " [ 59.12  44.  ]\n",
      " [196.82 205.  ]\n",
      " [ 54.81  54.  ]\n",
      " [ 39.73  39.  ]\n",
      " [ 32.06  32.  ]\n",
      " [132.32 141.  ]\n",
      " [119.05 124.  ]\n",
      " [ 58.41  72.  ]\n",
      " [ 50.16  50.  ]\n",
      " [ 40.    40.  ]\n",
      " [117.69  83.  ]\n",
      " [ 48.53  47.  ]]\n",
      "Random forests,Tesnsile strength\n",
      "K-fold validation score for 10 folds\n",
      "[0.91 0.89 0.91 0.85 0.86 0.96 0.91 0.94 0.9  0.88]\n",
      "mean\n",
      "0.8999256200654926\n",
      "std\n",
      "0.03153717691036756\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('../../Copper Dataset (version 2.0).csv')\n",
    "X = dataset.iloc[:, 0:118].values\n",
    "y = dataset.iloc[:, 118:119].values\n",
    "\n",
    "\n",
    "#trainingand testsplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 733, min_samples_split = 2, min_samples_leaf = 1, max_features = \"auto\", max_depth= 100, bootstrap = True, random_state = 0)\n",
    "regressor.fit(X_train, np.ravel(y_train))\n",
    "y_pred = regressor.predict(X_test)\n",
    "#For TS\n",
    "#n_estimators': 733, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 100, 'bootstrap': True\n",
    "\n",
    "#for T.C\n",
    "#n_estimators': 944, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 50, 'bootstrap': False\n",
    "\n",
    "#For new data extra 200 points\n",
    "#'n_estimators': 311, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "plt.scatter(y_test, y_pred, color='red')\n",
    "plt.plot(y_test,y_test, color='blue')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')\n",
    "plt.title('Tensile strength ')\n",
    "\n",
    "\n",
    "\n",
    "#Applying k-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator= regressor, X= X_train, y = y_train, cv= 10, n_jobs = -1 )\n",
    "A= accuracies.mean()\n",
    "B= accuracies.std()\n",
    "#Applying grid search to find best model and parameters\n",
    "print('Random forests,Tesnsile strength')\n",
    "print('K-fold validation score for 10 folds')\n",
    "print(accuracies)\n",
    "print('mean')\n",
    "print(A)\n",
    "print('std')\n",
    "print(B)\n",
    "\n",
    "\n",
    "\n",
    "##Hyper parametertuning from datascience website\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                 'max_features': max_features,\n",
    "#                 'max_depth': max_depth,\n",
    "#                 'min_samples_split': min_samples_split,\n",
    "#                 'min_samples_leaf': min_samples_leaf,\n",
    "#                 'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "# regressor = RandomForestRegressor()\n",
    "# regressor_random = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# regressor_random.fit(X_train, y_train)\n",
    "# L=regressor_random.best_params_\n",
    "# print(L)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
