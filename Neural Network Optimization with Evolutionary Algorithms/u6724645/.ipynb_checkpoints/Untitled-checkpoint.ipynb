{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(327)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(327)\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    \n",
    "    # Add categorical dummy variables (All 0s represent)\n",
    "    tasknum_dummies = pd.get_dummies(raw_data['Task_num'],\n",
    "                                     prefix=\"TaskNum\") # Create dummy variables\n",
    "    data = pd.concat([raw_data, tasknum_dummies], axis=1) \n",
    "\n",
    "    # Remove the unnecessary columns\n",
    "    remove_cols = [\"Skip_distance\",\n",
    "              \"Subject\",\n",
    "              \"Mean_fixation_duration\",\n",
    "              \"Loag_Fixationtime\",\n",
    "              \"Log_timetoF\",\n",
    "              \"Task_completion_duration\",\n",
    "              \"Compressed_scanpath_value\", \n",
    "              \"Total_r_d\",\n",
    "              \"Compressed_M_Minimal\",\n",
    "              \"Strictly_linearWID\",\n",
    "              \"Mean_fixation_duration_for_onelink\",\n",
    "              \"Skip\",\n",
    "              \"Skip_count\", \n",
    "              \"Task_num\",\n",
    "              \"TaskNum_t9\"]  # Remove one dummy variable to avoid the dummy variable trap\n",
    "\n",
    "    data = data.drop(remove_cols, axis=1)\n",
    "    \n",
    "    # Encode the Screen_size column\n",
    "    vals = ['S', 'M', 'L']\n",
    "    for i in range(len(vals)):\n",
    "        data.at[data['Screen_size'] == vals[i], ['Screen_size']] = i    \n",
    "\n",
    "    # Replace missing values with 0 in column Regression_distance\n",
    "    preprocessed_data = data.fillna(0)\n",
    "\n",
    "    # Inspect the number of missing values in the preprocessed_data dataset\n",
    "    num_missing = preprocessed_data.isnull().sum().sum()\n",
    "    print(\"The number of missing values in the data = {}\".format(num_missing))\n",
    "    print(\"Number of features remaining = {}\".format(data.shape[1]))\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the data = 0\n",
      "Number of features remaining = 30\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the dataset\n",
    "\n",
    "# Total number of columns in the dataset = 36\n",
    "required_cols = list(range(36))\n",
    "\n",
    "# Read the dataset\n",
    "raw_data = pd.read_excel(\"Jae-Second_Exp_data.xlsx\",\n",
    "                     sheet_name=\"Analysis_summary\",\n",
    "                     nrows=161,\n",
    "                     usecols = required_cols)\n",
    "\n",
    "# Step 2. Preprocess the data\n",
    "data = preprocess_data(raw_data = raw_data)\n",
    "\n",
    "# Step 3. Split the data into training and test sets\n",
    "\n",
    "# Divide into features and target variables\n",
    "X = data.drop(\"Screen_size\", axis=1)\n",
    "y = data['Screen_size']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalise training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_train_scaled_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "x_train = X_train_scaled_pca[:,[9,10,26,27]]\n",
    "x_test = X_test_scaled_pca[:,[9,10,26,27]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import best model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24 entries, 0 to 23\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   num_hidden_layers  24 non-null     float64\n",
      " 1   neurons_per_layer  24 non-null     float64\n",
      " 2   dropout_rate       24 non-null     float64\n",
      " 3   fitness_scores     24 non-null     float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 960.0 bytes\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"MY BEST NETWORKS\")\n",
    "\n",
    "best_models = data[data.fitness_scores > 0.467]\n",
    "best_models.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Activation Function = Softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network (using the parameters from best models)\n",
    "\n",
    "# Perform cross-validation on the network and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(num_hidden_layers, neurons_per_layer):\n",
    "#     activation_function = 'softsign'\n",
    "#     optimizer = 'adagrad'\n",
    "\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    classifier.add(Dense(units = neurons_per_layer, activation = activation_function, input_dim = x_train.shape[1]))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        classifier.add(Dense(units = neurons_per_layer, activation = activation_function))\n",
    "        classifier.add(Dropout(dropout, seed=327))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_fitness():\n",
    "    \n",
    "    model_params = {'num_hidden_layers':[num_hidden_layers], \n",
    "                    'neurons_per_layer':[neurons_per_layer]}    \n",
    "    # create model\n",
    "    model = KerasClassifier(build_fn=build_nn, epochs=20, batch_size=1, verbose=0)    \n",
    "    # Perform k-fold cross validation (using GridSearch here to reduce code size)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=327)\n",
    "    model_cv = GridSearchCV(estimator=model, param_grid=model_params, scoring='f1_weighted', cv=kfold, n_jobs=-1)\n",
    "    model_cv.fit(x_train, y_train)\n",
    "    \n",
    "    # Return weighted F1-score \n",
    "    return model_cv.cv_results_['mean_test_score'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(best_models)):\n",
    "\n",
    "    # Get parameters from individual\n",
    "    num_hidden_layers, neurons_per_layer, dropout = best_models.iloc[i,:-1].values\n",
    "    num_hidden_layers, neurons_per_layer = int(num_hidden_layers), int(neurons_per_layer)\n",
    "    activation_function = 'tanh'\n",
    "    optimizer = 'adagrad'\n",
    "    \n",
    "    results.append(get_individual_fitness())\n",
    "    print(i+1)\n",
    "    \n",
    "results_df = pd.DataFrame({'optimizer':[optimizer] * len(best_models),\n",
    "          'activation_function':[activation_function] * len(best_models),\n",
    "          'fitness_scores':results})\n",
    "\n",
    "a = pd.concat([best_models.drop('fitness_scores',axis=1), results_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_new(af, opt):\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(best_models)):\n",
    "\n",
    "        # Get parameters from individual\n",
    "        num_hidden_layers, neurons_per_layer, dropout = best_models.iloc[i,:-1].values\n",
    "        num_hidden_layers, neurons_per_layer = int(num_hidden_layers), int(neurons_per_layer)\n",
    "        activation_function = af\n",
    "        optimizer = opt\n",
    "\n",
    "        results.append(get_individual_fitness())\n",
    "        print(i+1)\n",
    "\n",
    "    results_df = pd.DataFrame({'optimizer':[optimizer] * len(best_models),\n",
    "              'activation_function':[activation_function] * len(best_models),\n",
    "              'fitness_scores':results})\n",
    "\n",
    "    a = pd.concat([best_models.drop('fitness_scores',axis=1), results_df], axis=1)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "final = pd.DataFrame()\n",
    "\n",
    "for af in ['relu', 'tanh', 'selu', 'softsign']:\n",
    "    for opt in ['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam']:\n",
    "        \n",
    "        a = approach_new(af, opt)\n",
    "        \n",
    "        final = final.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"best_models_and_activation_optimizers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>fitness_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.357395</td>\n",
       "      <td>sgd</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.376386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.199754</td>\n",
       "      <td>sgd</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.372747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.159685</td>\n",
       "      <td>sgd</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.381415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.172981</td>\n",
       "      <td>sgd</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.395266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>sgd</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.357958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.199754</td>\n",
       "      <td>nadam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.399536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.199754</td>\n",
       "      <td>nadam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.375168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>nadam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.362382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.208475</td>\n",
       "      <td>nadam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.402751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>nadam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.398723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_hidden_layers  neurons_per_layer  dropout_rate optimizer  \\\n",
       "0                 2.0               93.0      0.357395       sgd   \n",
       "1                 2.0               93.0      0.199754       sgd   \n",
       "2                 2.0               70.0      0.159685       sgd   \n",
       "3                 6.0               85.0      0.172981       sgd   \n",
       "4                 2.0               75.0      0.186952       sgd   \n",
       "..                ...                ...           ...       ...   \n",
       "19                3.0               87.0      0.199754     nadam   \n",
       "20                2.0               89.0      0.199754     nadam   \n",
       "21                2.0               85.0      0.014486     nadam   \n",
       "22                2.0               89.0      0.208475     nadam   \n",
       "23                4.0              110.0      0.186952     nadam   \n",
       "\n",
       "   activation_function  fitness_scores  \n",
       "0                 relu        0.376386  \n",
       "1                 relu        0.372747  \n",
       "2                 relu        0.381415  \n",
       "3                 relu        0.395266  \n",
       "4                 relu        0.357958  \n",
       "..                 ...             ...  \n",
       "19            softsign        0.399536  \n",
       "20            softsign        0.375168  \n",
       "21            softsign        0.362382  \n",
       "22            softsign        0.402751  \n",
       "23            softsign        0.398723  \n",
       "\n",
       "[672 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>fitness_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.172981</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.442106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>adamax</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.433926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.123161</td>\n",
       "      <td>sgd</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.427011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.172981</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.426135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.339876</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.419735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.123161</td>\n",
       "      <td>adam</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.338395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.172981</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.337989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>sgd</td>\n",
       "      <td>selu</td>\n",
       "      <td>0.337715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.208475</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>softsign</td>\n",
       "      <td>0.336033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.232044</td>\n",
       "      <td>sgd</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.318076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_hidden_layers  neurons_per_layer  dropout_rate optimizer  \\\n",
       "3                 6.0               85.0      0.172981   adagrad   \n",
       "4                 2.0               75.0      0.186952    adamax   \n",
       "8                 3.0               85.0      0.123161       sgd   \n",
       "3                 6.0               85.0      0.172981   adagrad   \n",
       "16                3.0               97.0      0.339876   rmsprop   \n",
       "..                ...                ...           ...       ...   \n",
       "8                 3.0               85.0      0.123161      adam   \n",
       "9                 5.0               75.0      0.172981  adadelta   \n",
       "12                2.0               85.0      0.014486       sgd   \n",
       "22                2.0               89.0      0.208475   rmsprop   \n",
       "10                3.0               82.0      0.232044       sgd   \n",
       "\n",
       "   activation_function  fitness_scores  \n",
       "3             softsign        0.442106  \n",
       "4                 selu        0.433926  \n",
       "8                 tanh        0.427011  \n",
       "3                 relu        0.426135  \n",
       "16            softsign        0.419735  \n",
       "..                 ...             ...  \n",
       "8             softsign        0.338395  \n",
       "9                 relu        0.337989  \n",
       "12                selu        0.337715  \n",
       "22            softsign        0.336033  \n",
       "10                tanh        0.318076  \n",
       "\n",
       "[672 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.sort_values(by='fitness_scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activation_function\n",
       "relu        0.337989\n",
       "selu        0.337715\n",
       "softsign    0.336033\n",
       "tanh        0.318076\n",
       "Name: fitness_scores, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.groupby('activation_function').min()['fitness_scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
