{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(327)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(327)\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    \n",
    "    # Add categorical dummy variables (All 0s represent)\n",
    "    tasknum_dummies = pd.get_dummies(raw_data['Task_num'],\n",
    "                                     prefix=\"TaskNum\") # Create dummy variables\n",
    "    data = pd.concat([raw_data, tasknum_dummies], axis=1) \n",
    "\n",
    "    # Remove the unnecessary columns\n",
    "    remove_cols = [\"Skip_distance\",\n",
    "              \"Subject\",\n",
    "              \"Mean_fixation_duration\",\n",
    "              \"Loag_Fixationtime\",\n",
    "              \"Log_timetoF\",\n",
    "              \"Task_completion_duration\",\n",
    "              \"Compressed_scanpath_value\", \n",
    "              \"Total_r_d\",\n",
    "              \"Compressed_M_Minimal\",\n",
    "              \"Strictly_linearWID\",\n",
    "              \"Mean_fixation_duration_for_onelink\",\n",
    "              \"Skip\",\n",
    "              \"Skip_count\", \n",
    "              \"Task_num\",\n",
    "              \"TaskNum_t9\"]  # Remove one dummy variable to avoid the dummy variable trap\n",
    "\n",
    "    data = data.drop(remove_cols, axis=1)\n",
    "    \n",
    "    # Encode the Screen_size column\n",
    "    vals = ['S', 'M', 'L']\n",
    "    for i in range(len(vals)):\n",
    "        data.at[data['Screen_size'] == vals[i], ['Screen_size']] = i    \n",
    "\n",
    "    # Replace missing values with 0 in column Regression_distance\n",
    "    preprocessed_data = data.fillna(0)\n",
    "\n",
    "    # Inspect the number of missing values in the preprocessed_data dataset\n",
    "    num_missing = preprocessed_data.isnull().sum().sum()\n",
    "    print(\"The number of missing values in the data = {}\".format(num_missing))\n",
    "    print(\"Number of features remaining = {}\".format(data.shape[1]))\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_model(hidden_neurons, learning_rate, num_epoch, input_neurons = 29, output_neurons=3):\n",
    "    \n",
    "    # define the structure of our neural network\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_neurons, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, hidden_neurons),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(hidden_neurons, output_neurons),\n",
    "    )\n",
    "\n",
    "    # define loss functions\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimiser\n",
    "    optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return net, loss_func, optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_neural_network(model, X_train, y_train):\n",
    "    \n",
    "    # create Tensors to hold inputs and outputs. Tensors are data structures\n",
    "    # similar to numpy matrices. They can be operated on efficiently by a GPU\n",
    "    # \n",
    "    # Note: In torch versions before 0.4, Tensors had to be wrapped in a Variable\n",
    "    # to be used by the NN.\n",
    "    X = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    Y = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    # store all losses for visualisation\n",
    "    all_losses = []\n",
    "\n",
    "    # train a neural network\n",
    "    for epoch in range(num_epoch):\n",
    "        # Perform forward pass: compute predicted y by passing x to the model.\n",
    "        # Here we pass a Tensor of input data to the Module and it produces\n",
    "        # a Tensor of output data.\n",
    "        # In this case, Y_pred contains three columns, where the index of the\n",
    "        # max column indicates the class of the instance\n",
    "        Y_pred = net(X)\n",
    "\n",
    "        # Compute loss\n",
    "        # Here we pass Tensors containing the predicted and true values of Y,\n",
    "        # and the loss function returns a Tensor containing the loss.\n",
    "        loss = loss_func(Y_pred, Y)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # print progress\n",
    "#         if epoch % 50 == 0:\n",
    "#             # convert three-column predicted Y values to one column for comparison\n",
    "#             _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "#             # calculate and print accuracy\n",
    "#             total = predicted.size(0)\n",
    "#             correct = predicted.data.numpy() == Y.data.numpy()\n",
    "\n",
    "#             print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "#                   % (epoch + 1, num_epoch, loss.item(), 100 * sum(correct)/total))\n",
    "\n",
    "        # Clear the gradients before running the backward pass.\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Perform backward pass: compute gradients of the loss with respect to\n",
    "        # all the learnable parameters of the model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimiser makes an update to its\n",
    "        # parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, x, y, mode='Train'):\n",
    "    \n",
    "    # Transform data to tensors\n",
    "    X = torch.tensor(x.values, dtype=torch.float)\n",
    "    Y = torch.tensor(y.values, dtype=torch.long) \n",
    "\n",
    "    # Create empty 3x3 confusion matrix\n",
    "    confusion = torch.zeros(3, 3)\n",
    "\n",
    "    # Make predictions on X\n",
    "    Y_pred = net(X)\n",
    "    _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "    # Create confusion Matrix\n",
    "    for i in range(X.shape[0]):\n",
    "        actual_class = Y.data[i]\n",
    "        predicted_class = predicted.data[i]\n",
    "\n",
    "        confusion[actual_class][predicted_class] += 1\n",
    "\n",
    "    # Calculate Accuracy score\n",
    "    correct_pred_count = confusion[0,0] + confusion[1,1] + confusion[2,2]\n",
    "    accuracy_score = correct_pred_count / confusion.sum() * 100\n",
    "\n",
    "    print(\"{}ing Accuracy = {}%\".format(mode, accuracy_score))    \n",
    "    print('Confusion matrix for {}ing:'.format(mode))\n",
    "    print(confusion.numpy())\n",
    "    \n",
    "    print(\"\\nClassification Report -\")\n",
    "    print(metrics.classification_report(y, predicted.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the data = 0\n",
      "Number of features remaining = 30\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the dataset\n",
    "\n",
    "# Total number of columns in the dataset = 36\n",
    "required_cols = list(range(36))\n",
    "\n",
    "# Read the dataset\n",
    "raw_data = pd.read_excel(\"Jae-Second_Exp_data.xlsx\",\n",
    "                     sheet_name=\"Analysis_summary\",\n",
    "                     nrows=161,\n",
    "                     usecols = required_cols)\n",
    "\n",
    "# Step 2. Preprocess the data\n",
    "data = preprocess_data(raw_data = raw_data)\n",
    "\n",
    "# Step 3. Split the data into training and test sets\n",
    "\n",
    "# Divide into features and target variables\n",
    "X = data.drop(\"Screen_size\", axis=1)\n",
    "y = data['Screen_size']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 4. Initialise model parameters\n",
    "hidden_neurons = 32\n",
    "learning_rate = 0.474\n",
    "num_epoch = 200\n",
    "\n",
    "# Step 5. Build model skeleton\n",
    "net, loss_func, optimiser = create_model(hidden_neurons=hidden_neurons,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         num_epoch=num_epoch)\n",
    "\n",
    "# Step 6. Train the model and store build history\n",
    "losses = train_neural_network(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 49.21875%\n",
      "Confusion matrix for Training:\n",
      "[[28.  6.  9.]\n",
      " [16. 10. 17.]\n",
      " [15.  2. 25.]]\n",
      "\n",
      "Classification Report -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.65      0.55        43\n",
      "           1       0.56      0.23      0.33        43\n",
      "           2       0.49      0.60      0.54        42\n",
      "\n",
      "    accuracy                           0.49       128\n",
      "   macro avg       0.51      0.49      0.47       128\n",
      "weighted avg       0.51      0.49      0.47       128\n",
      "\n",
      "Testing Accuracy = 42.42424392700195%\n",
      "Confusion matrix for Testing:\n",
      "[[6. 2. 3.]\n",
      " [6. 2. 3.]\n",
      " [5. 0. 6.]]\n",
      "\n",
      "Classification Report -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.55      0.43        11\n",
      "           1       0.50      0.18      0.27        11\n",
      "           2       0.50      0.55      0.52        11\n",
      "\n",
      "    accuracy                           0.42        33\n",
      "   macro avg       0.45      0.42      0.41        33\n",
      "weighted avg       0.45      0.42      0.41        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7. Evaluate on train and test data\n",
    "evaluate(net, X_train, y_train)\n",
    "evaluate(net, X_test, y_test, mode='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter tuning using Genetic Algorithm\n",
    "\n",
    "We wish to tune the following hyperparameters for our neural network architecture :\n",
    "1. Number of hidden layers\n",
    "2. Neurons per layer\n",
    "3. Activation Function for the hidden layers\n",
    "4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(num_hidden_layers, neurons_per_layer, activation_function, optimizer):\n",
    "\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    classifier.add(Dense(units = neurons_per_layer, activation = activation_function, input_dim = X_train.shape[1]))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        classifier.add(Dense(units = neurons_per_layer, activation = activation_function))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Initialise Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create the initial population\n",
    "def initialise_population(population_size):    \n",
    "    num_parameters = 4  # Number of hyperparameters we wish to tune    \n",
    "    # Initialize search space\n",
    "    population = np.zeros((population_size, num_parameters))    \n",
    "    # Define sample space for initial population\n",
    "    num_hidden_layers = np.arange(0, 10)\n",
    "    neurons_per_layer = np.arange(5, 100, 5)\n",
    "    activation_functions = np.array(['relu', 'tanh', 'selu', 'softsign'])    \n",
    "    optimizers = np.array(['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam'])    \n",
    "    # Add individuals to the population\n",
    "    for i in range(population_size):\n",
    "        # a) Randomly choose attributes from the search space\n",
    "        nhl = np.random.choice(num_hidden_layers)\n",
    "        npl = np.random.choice(neurons_per_layer)\n",
    "        af = np.random.choice(np.arange(len(activation_functions)))\n",
    "        opt = np.random.choice(np.arange(len(optimizers)))        \n",
    "        # b) Add individual with chosen attributes to the population\n",
    "        population[i,:] = nhl, npl, af, opt        \n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Get the fitness score of the members of a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary function: get_pop_fitness\n",
    "\n",
    "# Helper functions : \n",
    "    # a) translate_params\n",
    "    # b) build_nn\n",
    "    # c) get_individual_fitness\n",
    "\n",
    "def translate_params(params):\n",
    "    params = params.astype(int).tolist()\n",
    "    params[2] = activation_functions[params[2]]\n",
    "    params[3] = optimizers[params[3]]\n",
    "    return params\n",
    "\n",
    "def build_nn(num_hidden_layers, neurons_per_layer, activation_function, optimizer):\n",
    "\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    classifier.add(Dense(units = neurons_per_layer, activation = activation_function, input_dim = X_train.shape[1]))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        classifier.add(Dense(units = neurons_per_layer, activation = activation_function))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def get_individual_fitness(nn):\n",
    "    # Encode labels using dummy variables\n",
    "    y_train_nn = pd.get_dummies(y_train).values\n",
    "    # Fit the model\n",
    "    nn.fit(X_train, y_train_nn, batch_size = 10, epochs = 20, verbose=0)    \n",
    "    # Make predictions on test data\n",
    "    preds = nn.predict(X_test).argmax(axis=1)\n",
    "    # Return weighted F1-score \n",
    "    return metrics.f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "def get_pop_fitness(pop):\n",
    "    # Initialize empty list of fitness scores\n",
    "    fit_score = [] \n",
    "    # Loop over every member of the population\n",
    "    for individual in pop: \n",
    "        # Get parameters from individual\n",
    "        num_hidden_layers, neurons_per_layer, activation_function, optimizer = translate_params(individual)\n",
    "        # Build a neural network wrt the member's parameters         \n",
    "        nn = build_nn(num_hidden_layers, neurons_per_layer, activation_function, optimizer)  \n",
    "        # Calculate the fitness of the member\n",
    "        fitness = get_individual_fitness(nn)\n",
    "        # Add member fitness to record of population fitness\n",
    "        fit_score.append(fitness)\n",
    "    # Return the list of fitness scores of the entire population  \n",
    "    return fit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 3. Create the initial population\n",
    "def initialise_population(population_size):    \n",
    "    num_parameters = 4  # Number of hyperparameters we wish to tune    \n",
    "    # Initialize search space\n",
    "    population = np.zeros((population_size, num_parameters))    \n",
    "    # Define sample space for initial population\n",
    "    num_hidden_layers = np.arange(0, 10)\n",
    "    neurons_per_layer = np.arange(5, 100, 5)\n",
    "    activation_functions = np.array(['relu', 'tanh', 'selu', 'softsign'])    \n",
    "    optimizers = np.array(['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam'])    \n",
    "    # Add individuals to the population\n",
    "    for i in range(population_size):\n",
    "        # a) Randomly choose attributes from the search space\n",
    "        nhl = np.random.choice(num_hidden_layers)\n",
    "        npl = np.random.choice(neurons_per_layer)\n",
    "        af = np.random.choice(np.arange(len(activation_functions)))\n",
    "        opt = np.random.choice(np.arange(len(optimizers)))        \n",
    "        # b) Add individual with chosen attributes to the population\n",
    "        population[i,:] = nhl, npl, af, opt        \n",
    "    return population\n",
    "\n",
    "population_size = 20\n",
    "population = initialise_population(population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 4. Calculate population fitness for an entire generation\n",
    "\n",
    "# Primary function: get_pop_fitness\n",
    "\n",
    "# Helper functions : \n",
    "    # a) translate_params\n",
    "    # b) build_nn\n",
    "    # c) get_individual_fitness\n",
    "\n",
    "def translate_params(params):\n",
    "    params = params.astype(int).tolist()\n",
    "    params[2] = activation_functions[params[2]]\n",
    "    params[3] = optimizers[params[3]]\n",
    "    return params\n",
    "\n",
    "def build_nn(num_hidden_layers, neurons_per_layer, activation_function, optimizer):\n",
    "\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    classifier.add(Dense(units = neurons_per_layer, activation = activation_function, input_dim = X_train.shape[1]))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        classifier.add(Dense(units = neurons_per_layer, activation = activation_function))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def get_individual_fitness(nn):\n",
    "    # Encode labels using dummy variables\n",
    "    y_train_nn = pd.get_dummies(y_train).values\n",
    "    # Fit the model\n",
    "    nn.fit(X_train, y_train_nn, batch_size = 10, epochs = 20, verbose=0)    \n",
    "    # Make predictions on test data\n",
    "    preds = nn.predict(X_test).argmax(axis=1)\n",
    "    # Return weighted F1-score \n",
    "    return metrics.f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "def get_pop_fitness(pop):\n",
    "    # Initialize empty list of fitness scores\n",
    "    fit_score = [] \n",
    "    # Loop over every member of the population\n",
    "    for individual in pop: \n",
    "        # Get parameters from individual\n",
    "        num_hidden_layers, neurons_per_layer, activation_function, optimizer = translate_params(individual)\n",
    "        # Build a neural network wrt the member's parameters         \n",
    "        nn = build_nn(num_hidden_layers, neurons_per_layer, activation_function, optimizer)  \n",
    "        # Calculate the fitness of the member\n",
    "        fitness = get_individual_fitness(nn)\n",
    "        # Add member fitness to record of population fitness\n",
    "        fit_score.append(fitness)\n",
    "    # Return the list of fitness scores of the entire population  \n",
    "    return fit_score\n",
    "\n",
    "# Test code for Fitness functions\n",
    "get_pop_fitness(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Represent parameters as a chromosome\n",
    "\n",
    "# Step 2. Defining the population size and number of generations\n",
    "POPULATION_SIZE = 20\n",
    "NUM_GENERATIONS = 5\n",
    "\n",
    "# Step 3. Creating the initial population.\n",
    "population = initialise_population(POPULATION_SIZE)\n",
    "\n",
    "# Repeat for each generation the following\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    \n",
    "    # Step 4. Measure the fitness of each chromosome in the population.\n",
    "                # by Training and evaluating (f1-scores) for all networks in the population:\n",
    "    fitness_scores = get_pop_fitness(population)\n",
    "    \n",
    "    # Step 5. Selecting the best parents in the population for mating.\n",
    "    \n",
    "    # Step 6. Generating next generation using crossover.\n",
    "\n",
    "    # Step 7. Adding some variations to the offsrping using mutation.\n",
    "    \n",
    "    # Step 8. Creating the new population based on the parents and offspring.\n",
    "    \n",
    "    # Step 9. Displaying the best result in the current generation    \n",
    "\n",
    "# Get the best solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
