{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducible results\n",
    "from numpy.random import seed\n",
    "seed(327)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(327)\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    \n",
    "    # Add categorical dummy variables (All 0s represent)\n",
    "    tasknum_dummies = pd.get_dummies(raw_data['Task_num'],\n",
    "                                     prefix=\"TaskNum\") # Create dummy variables\n",
    "    data = pd.concat([raw_data, tasknum_dummies], axis=1) \n",
    "\n",
    "    # Remove the unnecessary columns\n",
    "    remove_cols = [\"Skip_distance\",\n",
    "              \"Subject\",\n",
    "              \"Mean_fixation_duration\",\n",
    "              \"Loag_Fixationtime\",\n",
    "              \"Log_timetoF\",\n",
    "              \"Task_completion_duration\",\n",
    "              \"Compressed_scanpath_value\", \n",
    "              \"Total_r_d\",\n",
    "              \"Compressed_M_Minimal\",\n",
    "              \"Strictly_linearWID\",\n",
    "              \"Mean_fixation_duration_for_onelink\",\n",
    "              \"Skip\",\n",
    "              \"Skip_count\", \n",
    "              \"Task_num\",\n",
    "              \"TaskNum_t9\"]  # Remove one dummy variable to avoid the dummy variable trap\n",
    "\n",
    "    data = data.drop(remove_cols, axis=1)\n",
    "    \n",
    "    # Encode the Screen_size column\n",
    "    vals = ['S', 'M', 'L']\n",
    "    for i in range(len(vals)):\n",
    "        data.at[data['Screen_size'] == vals[i], ['Screen_size']] = i    \n",
    "\n",
    "    # Replace missing values with 0 in column Regression_distance\n",
    "    preprocessed_data = data.fillna(0)\n",
    "\n",
    "    # Inspect the number of missing values in the preprocessed_data dataset\n",
    "    num_missing = preprocessed_data.isnull().sum().sum()\n",
    "    print(\"The number of missing values in the data = {}\".format(num_missing))\n",
    "    print(\"Number of features remaining = {}\".format(data.shape[1]))\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_model(hidden_neurons, learning_rate, num_epoch, input_neurons = 29, output_neurons=3):\n",
    "    \n",
    "    # define the structure of our neural network\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_neurons, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, hidden_neurons),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(hidden_neurons, output_neurons),\n",
    "    )\n",
    "\n",
    "    # define loss functions\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimiser\n",
    "    optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return net, loss_func, optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_neural_network(model, X_train, y_train):\n",
    "    \n",
    "    # create Tensors to hold inputs and outputs. Tensors are data structures\n",
    "    # similar to numpy matrices. They can be operated on efficiently by a GPU\n",
    "    # \n",
    "    # Note: In torch versions before 0.4, Tensors had to be wrapped in a Variable\n",
    "    # to be used by the NN.\n",
    "    X = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    Y = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    # store all losses for visualisation\n",
    "    all_losses = []\n",
    "\n",
    "    # train a neural network\n",
    "    for epoch in range(num_epoch):\n",
    "        # Perform forward pass: compute predicted y by passing x to the model.\n",
    "        # Here we pass a Tensor of input data to the Module and it produces\n",
    "        # a Tensor of output data.\n",
    "        # In this case, Y_pred contains three columns, where the index of the\n",
    "        # max column indicates the class of the instance\n",
    "        Y_pred = net(X)\n",
    "\n",
    "        # Compute loss\n",
    "        # Here we pass Tensors containing the predicted and true values of Y,\n",
    "        # and the loss function returns a Tensor containing the loss.\n",
    "        loss = loss_func(Y_pred, Y)\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        # print progress\n",
    "#         if epoch % 50 == 0:\n",
    "#             # convert three-column predicted Y values to one column for comparison\n",
    "#             _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "#             # calculate and print accuracy\n",
    "#             total = predicted.size(0)\n",
    "#             correct = predicted.data.numpy() == Y.data.numpy()\n",
    "\n",
    "#             print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "#                   % (epoch + 1, num_epoch, loss.item(), 100 * sum(correct)/total))\n",
    "\n",
    "        # Clear the gradients before running the backward pass.\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Perform backward pass: compute gradients of the loss with respect to\n",
    "        # all the learnable parameters of the model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimiser makes an update to its\n",
    "        # parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, x, y, mode='Train'):\n",
    "    \n",
    "    # Transform data to tensors\n",
    "    X = torch.tensor(x.values, dtype=torch.float)\n",
    "    Y = torch.tensor(y.values, dtype=torch.long) \n",
    "\n",
    "    # Create empty 3x3 confusion matrix\n",
    "    confusion = torch.zeros(3, 3)\n",
    "\n",
    "    # Make predictions on X\n",
    "    Y_pred = net(X)\n",
    "    _, predicted = torch.max(F.softmax(Y_pred,1), 1)\n",
    "\n",
    "    # Create confusion Matrix\n",
    "    for i in range(X.shape[0]):\n",
    "        actual_class = Y.data[i]\n",
    "        predicted_class = predicted.data[i]\n",
    "\n",
    "        confusion[actual_class][predicted_class] += 1\n",
    "\n",
    "    # Calculate Accuracy score\n",
    "    correct_pred_count = confusion[0,0] + confusion[1,1] + confusion[2,2]\n",
    "    accuracy_score = correct_pred_count / confusion.sum() * 100\n",
    "\n",
    "    print(\"{}ing Accuracy = {}%\".format(mode, accuracy_score))    \n",
    "    print('Confusion matrix for {}ing:'.format(mode))\n",
    "    print(confusion.numpy())\n",
    "    \n",
    "    print(\"\\nClassification Report -\")\n",
    "    print(metrics.classification_report(y, predicted.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the data = 0\n",
      "Number of features remaining = 30\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the dataset\n",
    "\n",
    "# Total number of columns in the dataset = 36\n",
    "required_cols = list(range(36))\n",
    "\n",
    "# Read the dataset\n",
    "raw_data = pd.read_excel(\"Jae-Second_Exp_data.xlsx\",\n",
    "                     sheet_name=\"Analysis_summary\",\n",
    "                     nrows=161,\n",
    "                     usecols = required_cols)\n",
    "\n",
    "# Step 2. Preprocess the data\n",
    "data = preprocess_data(raw_data = raw_data)\n",
    "\n",
    "# Step 3. Split the data into training and test sets\n",
    "\n",
    "# Divide into features and target variables\n",
    "X = data.drop(\"Screen_size\", axis=1)\n",
    "y = data['Screen_size']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4. Initialise model parameters\n",
    "# hidden_neurons = 32\n",
    "# learning_rate = 0.474\n",
    "# num_epoch = 200\n",
    "\n",
    "# # Step 5. Build model skeleton\n",
    "# net, loss_func, optimiser = create_model(hidden_neurons=hidden_neurons,\n",
    "#                                          learning_rate=learning_rate,\n",
    "#                                          num_epoch=num_epoch)\n",
    "\n",
    "# # Step 6. Train the model and store build history\n",
    "# losses = train_neural_network(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Step 7. Evaluate on train and test data\n",
    "# evaluate(net, X_train, y_train)\n",
    "# evaluate(net, X_test, y_test, mode='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter tuning using Genetic Algorithm\n",
    "\n",
    "We wish to tune the following hyperparameters for our neural network architecture :\n",
    "1. Number of hidden layers\n",
    "2. Neurons per layer\n",
    "3. Activation Function for the hidden layers\n",
    "4. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Initialise Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create the initial population\n",
    "def initialise_population(population_size):    \n",
    "    num_parameters = 3  # Number of hyperparameters we wish to tune    \n",
    "    # Initialize search space\n",
    "    population = np.zeros((population_size, num_parameters))    \n",
    " \n",
    "    # Add individuals to the population\n",
    "    for i in range(population_size):\n",
    "        # a) Randomly choose attributes from the search space\n",
    "        nhl = np.random.choice(num_hidden_layers)\n",
    "        npl = np.random.choice(neurons_per_layer)\n",
    "        dpt = np.random.choice(dropout)\n",
    "        # b) Add individual with chosen attributes to the population\n",
    "        population[i,:] = nhl, npl, dpt        \n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Get the fitness score of the members of a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary function: get_pop_fitness\n",
    "\n",
    "# Helper functions : \n",
    "    # a) translate_params\n",
    "    # b) build_nn\n",
    "    # c) get_individual_fitness\n",
    "\n",
    "def translate_params(params):\n",
    "    params = params.tolist()\n",
    "    params[0] = int(params[0])\n",
    "    params[1] = int(params[1])\n",
    "    params[2] = params[2]\n",
    "    return params\n",
    "\n",
    "def build_nn(num_hidden_layers, neurons_per_layer, dropout):\n",
    "    \n",
    "    activation_function, optimizer = 'relu', 'adam'\n",
    "    \n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add first hidden layer\n",
    "    classifier.add(Dense(units = neurons_per_layer, activation = activation_function, input_dim = X_train.shape[1]))\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        classifier.add(Dense(units = neurons_per_layer, activation = activation_function))\n",
    "        classifier.add(Dropout(dropout, seed=327))\n",
    "        \n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def get_individual_fitness(individual):\n",
    "    \n",
    "    # Get parameters from individual\n",
    "    num_hidden_layers, neurons_per_layer, dropout = translate_params(individual)\n",
    "    model_params = {'num_hidden_layers':[num_hidden_layers], \n",
    "                    'neurons_per_layer':[neurons_per_layer],\n",
    "                    'dropout':[dropout]}    \n",
    "    # create model\n",
    "    model = KerasClassifier(build_fn=build_nn, epochs=20, batch_size=1, verbose=0)    \n",
    "    # Perform k-fold cross validation (using GridSearch here to reduce code size)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=327)\n",
    "    model_cv = GridSearchCV(estimator=model, param_grid=model_params, scoring='f1_weighted', cv=kfold, n_jobs=-1)\n",
    "    model_cv.fit(X_train, y_train)\n",
    "    # Return weighted F1-score \n",
    "    return model_cv.cv_results_['mean_test_score'][0]\n",
    "\n",
    "def get_pop_fitness(pop):\n",
    "    # Initialize empty list of fitness scores\n",
    "    fit_score = [] \n",
    "    # Loop over every member of the population\n",
    "    for individual in pop: \n",
    "        fitness = get_individual_fitness(individual)\n",
    "        # Add member fitness to record of population fitness\n",
    "        fit_score.append(fitness)\n",
    "    # Return the list of fitness scores of the entire population  \n",
    "    return np.array(fit_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E. Selecting the best parents in the population for mating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_fittest_individuals(population, fitness_scores, n):\n",
    "    return population[fitness_scores.argsort()[::-1]][:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F. Reproduction (Single-Point-Crossover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Generating next generation using crossover\n",
    "\n",
    "def single_point_crossover(fittest_individuals, n):\n",
    "    children = []\n",
    "    # Generate all possible pairs of Fit Individuals for reproduction\n",
    "    for a, b in itertools.combinations(np.arange(len(fittest_individuals)), 2):        \n",
    "        # Initialise parents\n",
    "        parent_A = fittest_individuals[a].copy()\n",
    "        parent_B = fittest_individuals[b].copy()\n",
    "        # Randomly select a gene to be switched between both parents\n",
    "        crossover_idx = np.random.randint(0, 3)\n",
    "        # Switch that gene between the parents\n",
    "        parent_B[crossover_idx], parent_A[crossover_idx] = parent_A[crossover_idx], parent_B[crossover_idx]\n",
    "        # These modified parents are the children for the next generation\n",
    "        children.append(parent_A)\n",
    "        children.append(parent_B)\n",
    "    \n",
    "    # Indices of all the children \n",
    "    all_child_ids = np.arange(len(children))\n",
    "    # Randomly select 'n' children that survive to the next generation\n",
    "    survived_child_ids = np.random.choice(all_child_ids, n, replace=False).astype(int)\n",
    "    children = np.array(children)[survived_child_ids]\n",
    "    \n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### G. Mutation (all children are mutated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7. Adding some variations to the new generation\n",
    "def mutate(survivors):\n",
    "    \n",
    "    m_survivors = survivors.copy()\n",
    "    \n",
    "    for survivor in m_survivors:\n",
    "        # Randomly select a gene to mutate\n",
    "        gene_id = np.random.randint(0, 3)\n",
    "        \n",
    "        # Determine if the mutation is an addition/subtraction operation (50-50 chance of either)\n",
    "        p = 0.5\n",
    "        sign = 1 if (np.random.random() > p) else -1\n",
    "\n",
    "        # Mutate the genes (0 or 1) using values drawn from a gaussian distribution\n",
    "        if gene_id == 0:\n",
    "            factor = 3\n",
    "            mutation = int(np.random.random() * factor)\n",
    "            survivor[gene_id] += mutation    \n",
    "            if survivor[gene_id] < 0:\n",
    "                survivor[gene_id] = 0\n",
    "\n",
    "        if gene_id == 1:\n",
    "            factor = 10\n",
    "            mutation = int(np.random.random() * factor)\n",
    "            survivor[gene_id] += mutation\n",
    "            if survivor[gene_id] <= 0:\n",
    "                survivor[gene_id] = 2\n",
    "                \n",
    "        if gene_id == 2:\n",
    "            factor = 10\n",
    "            mutation = int(np.random.random() / factor)\n",
    "            survivor[gene_id] += mutation\n",
    "            if survivor[gene_id] <= 0:\n",
    "                survivor[gene_id] = 0\n",
    "            elif survivor[gene_id] >= 1:\n",
    "                survivor[gene_id] = 0.99\n",
    "    \n",
    "    return m_survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample space for initial population\n",
    "num_hidden_layers = np.arange(0, 30)\n",
    "neurons_per_layer = np.arange(5, 100, 5)\n",
    "# activation_functions = np.array(['relu', 'tanh', 'selu', 'softsign'])    \n",
    "# optimizers = np.array(['sgd', 'rmsprop', 'adam', 'adadelta', 'adagrad', 'adamax', 'nadam'])   \n",
    "dropout = np.random.random(size=30) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 30, 0.25043158854536307] 0.32806031891009885\n",
      "[6, 33, 0.009832083722916551] 0.391293127793061\n",
      "Wall time: 28min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 1. Represent parameters as a chromosome\n",
    "\n",
    "# Step 2. Defining the population size and number of generations\n",
    "POPULATION_SIZE = 20\n",
    "NUM_GENERATIONS = 2\n",
    "\n",
    "NUM_PARENTS_PER_GENERATION = 5\n",
    "NUM_CHILDREN_PER_GENRATION = 10\n",
    "NUM_WEAK_SURVIVORS = POPULATION_SIZE - (NUM_PARENTS_PER_GENERATION + NUM_CHILDREN_PER_GENRATION)\n",
    "\n",
    "# Book keeping variables\n",
    "BEST_PERFORMERS = []\n",
    "BEST_PERFORMERS_FITNESS = []\n",
    "\n",
    "# Step 3. Creating the initial population.\n",
    "population = initialise_population(POPULATION_SIZE)\n",
    "\n",
    "# Repeat for each generation the following\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    \n",
    "    old_population = population\n",
    "    \n",
    "    # Step 4. Measure the fitness of each chromosome in the population.\n",
    "                # by Training and evaluating (f1-scores) for all networks in the population:\n",
    "    fitness_scores = get_pop_fitness(old_population)\n",
    "    \n",
    "    # Step 5. Selecting the best parents in the population for mating.\n",
    "    fittest_individuals = select_fittest_individuals(old_population, fitness_scores, NUM_PARENTS_PER_GENERATION)\n",
    "    \n",
    "    BEST_PERFORMERS.append(fittest_individuals)\n",
    "    BEST_PERFORMERS_FITNESS.append(np.sort(fitness_scores)[::-1][:len(fittest_individuals)])\n",
    "    \n",
    "    # Step 6. Generating next generation using single-point crossover.\n",
    "    children = single_point_crossover(fittest_individuals, NUM_CHILDREN_PER_GENRATION)\n",
    "    \n",
    "    # Step 7. Adding some variations to the population using mutation on children and few weak members.\n",
    "    children = mutate(children)\n",
    "    weak_survivors = old_population[fitness_scores.argsort()][:NUM_WEAK_SURVIVORS] # Members with least fitness scores\n",
    "    weak_survivors = mutate(weak_survivors)\n",
    "    \n",
    "    # Step 8. Creating the new population (Fittest, Weakest, Children)\n",
    "    population = np.concatenate((fittest_individuals, children, weak_survivors), axis=0)\n",
    "    \n",
    "    # Step 9. Displaying the best result in the current generation\n",
    "    generation_best_individual = old_population[fitness_scores.argmax()]\n",
    "    generation_best_individual = translate_params(generation_best_individual)\n",
    "    print(generation_best_individual, fitness_scores.max())\n",
    "\n",
    "# Get the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"Parents from EA_continuation(Add Dropout)\", BEST_PERFORMERS)\n",
    "# np.save(\"Fitness from EA_continuation(Add Dropout)\", BEST_PERFORMERS_FITNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(BEST_PERFORMERS).reshape((-1, 3))\n",
    "bf = np.array(BEST_PERFORMERS_FITNESS).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hidden_layers</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>fitness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.391293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.349131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.250432</td>\n",
       "      <td>0.328060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.325042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.250432</td>\n",
       "      <td>0.325010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.304513</td>\n",
       "      <td>0.313245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.304513</td>\n",
       "      <td>0.310340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.292505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.264076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.246116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_hidden_layers  neurons_per_layer   dropout  fitness_score\n",
       "5                6.0               33.0  0.009832       0.391293\n",
       "6                7.0               70.0  0.243635       0.349131\n",
       "0                5.0               30.0  0.250432       0.328060\n",
       "7                6.0               10.0  0.009832       0.325042\n",
       "8                5.0               72.0  0.250432       0.325010\n",
       "1                3.0               10.0  0.304513       0.313245\n",
       "9                3.0                8.0  0.304513       0.310340\n",
       "2               11.0               55.0  0.081914       0.292505\n",
       "3                7.0               70.0  0.243635       0.264076\n",
       "4                6.0                5.0  0.009832       0.246116"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fit = pd.DataFrame(b, columns=['num_hidden_layers', 'neurons_per_layer', 'dropout'])\n",
    "df_fit['fitness_score'] = bf\n",
    "df_fit = df_fit.sort_values(by='fitness_score', ascending=False)\n",
    "df_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
